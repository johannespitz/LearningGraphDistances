\section{Approach}

\subsection{Experiment 1}

At first we will train kernel classifiers on MNIST and CIFAR-10 using two different approaches. One will be called \textit{interpolated} and the other \textit{overfitted}.

Let $K(\mathbf{x}, \mathbf{z}) : \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ be positive definite. Given data $\{(\mathbf{x}_i, y_i), i=1,...,n\}, \mathbf{x}_i \in \mathbb{R}^d, y_i \in \mathbb{R}$, then there exists a Reproducing Kernel Hilbert Space (RKHS) $\mathscr{H}$ of functions $f$ on $\mathbb{R}^d$. Moreover let $\mathbf{K}$ be the kernel matrix with $\mathbf{K}_{ij} = K(x_i, x_j)$. Then we are looking for the function $f^*$ which correctly classifies all training samples and minimizes the RKHS norm.

\begin{equation}
    f(\cdot) = \sum_{i=1}^{n} \alpha_{i} K(x_{i} ,\cdot)
\end{equation}
\begin{equation}
    f^* = \argmin_{f \in \mathscr{H}, f(\mathbf{x}_i)=y_i} \norm{f}_\mathscr{H} \text{, where } \norm{f}_\mathscr{H}^2 = \langle \boldsymbol{\alpha}, \mathbf{K} \boldsymbol{\alpha} \rangle = \sum_{ij} \alpha_i \mathbf{K}_{ij} \alpha_j
\end{equation}




\textbf{Interpolated:} By the classical representer theorem we get an explicit form to solve for the optimal $\alpha_i$.
\begin{equation}
    \boldsymbol{\alpha^*} =  \mathbf{K}^{-1} \mathbf{y}
\end{equation}
Note when we plug in the training data $X$ into the function $f^*$ we can see that indeed all samples are classified correctly.
\begin{equation}
\begin{aligned}
    f^*(\mathbf{X}) &= \mathbf{K} \boldsymbol{\alpha^*} \\
    &= \mathbf{K} \mathbf{K}^{-1} \mathbf{y} \\
    % &= \mathbf{y} \\
    f^*(\mathbf{x}_i) &= y_i \\
\end{aligned}
\end{equation}

\textbf{Overfitted:} To avoid having to solve the linear system which is prohibitive for larger datasets we can also pick any non-negative and strictly convex loss function $l$, such that $l(y, y) = 0$, and solve the unconstrained optimization problem.
\begin{equation}
    \boldsymbol{\alpha^*} = \argmin_{\boldsymbol{\alpha} \in \mathbb{R}} \sum_{j=1}^{N} l(\sum_{i=1}^{N} \alpha_{i} K(x_{i} ,x_{j}), y_i)
\end{equation}
Since $f^*$ minimizes $\sum_{j=1}^{N} l(f(x_i), y_i)$, this allows us to use gradient descent methods, and therefore shows the resemblance to a neural network. Basically, the weights between the input and the first layer are fixed (all the features of all training samples). This first layer has $N$ neurons, where $N$ is the number of training samples. The $\alpha_i$ are then the trainable weights connecting the first layer with the output layer.

\subsection{Experiment 2}

For the second experiment we will additionally use a synthetic datasets, and add label noise to both synthetic and real datasets.


\textbf{Adding noise:} We will randomly flip the true label of a $\epsilon$-fraction of the training and test samples to any of the possible labels with equal probability. Note that the Bayes Optimal Classifier remains the same on this new dataset and that the error rate scales linearly in $\epsilon$ between the error rate on the original dataset and random guessing (confer to Proportion 1 in Belkin et al. \cite{belkin2018a} for more details).

\textbf{Synthetic dataset:} The synthetic dataset has features $x \in \mathbb{R}^{50}$ and labels $y \in \{0, 1\}$. While $x_2 \sim \mathcal{U}(-1, 1), ..., x_{50} \sim \mathcal{U}(-1, 1)$ are drawn from the same uniform distribution, $x_1$ is drawn from a normal distribution depending on the class label.

\begin{equation}
x_1 \in \left\{
\begin{array}{@{}ll@{}}
    \mathcal{N}(0, 1), & \text{if}\ y=1 \\
    \mathcal{N}(2, 1), & \text{otherwise}
\end{array}\right.
\end{equation}

Note that the error of the Bayes Optimal Classifier is around $15.9\%$ for this dataset (the classes are not separable).


\subsection{General}

Throughout this report we will use two different kernels, the smooth Gaussain kernel $K(x,y) = \exp \left( - \frac{\norm{x-y}^2}{2\sigma^2} \right)$ and the non-smooth Laplacian kernel $K(x,y) = \exp \left( - \frac{\norm{x-y}}{\sigma} \right)$. Most of the overfitted examples are trained using the iterative EigenPro-SGD method by Ma and Belkin \cite{ma2017}. EigenPro is a preconditioned gradient descent iteration especially designed for fast convergence in kernel learning. In one occasion we used the Pegasos optimizer \cite{pegasos} because the EigenPro optimizer consistently converged towards random guessing on the training data instead of overfitting it. Both optimizer are available in the Keras Deep Learning library. The code for all experiments can be found on github (\href{https://github.com/johannespitz/kernel-overfitting}{link}).
