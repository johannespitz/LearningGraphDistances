\section{Derivation of the model}

% derivation
%     (cost_matrix -> matching) premutaiton invariant (matching is because invariant)

%     GED
%     compare two graphs

%     try matching

%     discrete -> non differentiable
%     -> EMD slow

%     sinkhorn

%     matrix scaling

%     backprop in optimal (Danskin)

%     (nystrom, multiscale sinkhorn)
%     (lanmarks, cluster)

%     GNN
%     Consensus



% (assume E << V^2)
We want our model to be applicable to any set of graphs $\mathcal{G}$, where each graph $G = \{V, E\} \in \mathcal{G}$ with nodes $V$ and edges $E$ is drawn from a common distribution. The nodes and edges may have arbitrary features. Possible distributions include chemical molecules, control flow graphs generated by C++ compilers, or preferential attachment graphs (\citealp{pref_att2002}).

The Graph Edit Distance (GED) between two graphs is defined analog to the String Edit Distance.
\begin{equation}
     \text{GED}(G_{1},G_{2}) = \min_{(e_{1},...,e_{k}) \in \mathcal{P}(g_{1},g_{2})} \sum_{i=1}^{k} c(e_{i})
\end{equation}
where $e_{i}$ are edit operations: edge/node addtion, removal, and substituion, $c(e)$ the cost of an edit operation, and $\mathcal{P}(G_{1},G_{2})$ the edit paths that transform $G_{1}$ into a graph isomorphic to $G_{2}$.

Note that the GED is symetric and invariant to the graph representation. Clearly the GED as defined above cannot depend on the ordering in which the nodes are saved in memory. Moreover, as long as the costs of substituion are symetric, also the GED is symetric ($\text{GED}(G_{1},G_{2}) = \text{GED}(G_{2},G_{1})$). Therefore, a neural network approximating the GED should also be symetric and invariant to the graph representation.

The simplest idea might be to embed both graphs into a common vector space and compute the distance between these two vectors. One can use message passing networks to generate node embeddings. Message passing is invariant to the graph representation and is commom practice in machine learning on graphs due to efficient implemention and great performance. These node embeddings can then be aggregated into a single graph embedding using any aggregation method, possibly combined with an attention layer. Finally, any distance between two vectors (cosine, $p$-norm) will ensure symetric predictions. The main problem with this approach is that the embedding size becomes a bottleneck for larger graphs and the entire model would need to be trained from scratch to increase it.
%  \cite{li2019}, and \cite{bai2019}

To solve the problem we match nodes of the two graphs, similar to classical GED algorithm (\citealp{hungarian2009}; \citealp{frankhauser2011}). These algorithms are based on bipartite graph matching. They set up a cost Matrix $C \in \mathbb{R}^{N \times N}$, where $N = \vert V_1 \vert + \vert V_2 \vert$ and solve the following contrained optimization problem.

% \begin{equation}
%      \begin{gathered}
%           \min \sum_{i = 1}^{N'} \sum_{j = 1}^{N'} T_{ij} C_{ij} \\
%           \text{subject to} \\
%           \sum_{i = 1}^{N'} T_{ij} = 1 \forall j \in \{1 \dots N'\} \\
%           \sum_{j = 1}^{N'} T_{ij} = 1 \forall i \in \{1 \dots N'\} \\
%           T_{ij} \in \{0, 1\}
%      \end{gathered}
% \end{equation}

% \begin{equation}
%      C=
%           \left[
%           \begin{array}{ccc|ccc}
%                C_{1,1} & \dotsi & C_{1, N_2} & C_{1, \epsilon} & \dotsi & \infty \\
%                \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
%                C_{N_1, 1} & \dotsi & C_{N_1, N_2} & \infty & \dotsi & C_{N_1, \epsilon} \\
%                \hline
%                C_{\epsilon, 1} & \dotsi & \infty & 0 & \dotsi & 0 \\
%                \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
%                \infty & \dotsi & C_{\epsilon, N_2} & 0 & \dotsi & 0 \\
%           \end{array}
%           \right]
% \end{equation}


\noindent
\begin{minipage}{.5\linewidth}

     \[
          C=
               \left[
               \begin{array}{ccc|ccc}
                    C_{1,1} & \dotsi & C_{1, N_2} & C_{1, \epsilon} & \dotsi & \infty \\
                    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
                    C_{N_1, 1} & \dotsi & C_{N_1, N_2} & \infty & \dotsi & C_{N_1, \epsilon} \\
                    \hline
                    C_{\epsilon, 1} & \dotsi & \infty & 0 & \dotsi & 0 \\
                    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
                    \infty & \dotsi & C_{\epsilon, N_2} & 0 & \dotsi & 0 \\
               \end{array}
               \right]
     \]

\end{minipage}%
\begin{minipage}{.5\linewidth}

     \begin{equation}
          \begin{gathered}
               \min \sum_{i = 1}^{N'} \sum_{j = 1}^{N'} T_{ij} C_{ij} \\
               \text{subject to} \\
               \sum_{i = 1}^{N'} T_{ij} = 1 \forall j \in \{1 \dots N'\} \\
               \sum_{j = 1}^{N'} T_{ij} = 1 \forall i \in \{1 \dots N'\} \\
               T_{ij} \in \{0, 1\}
          \end{gathered}
     \end{equation}

\end{minipage}


% \begin{equation}
%      C=
%           \left[
%           \begin{array}{ccc|ccc}
%                C_{1,1} & \dotsi & C_{1, N_2} & C_{1, \epsilon} & \dotsi & C_{1, \epsilon} \\
%                \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
%                C_{N_1, 1} & \dotsi & C_{N_1, N_2} & C_{N_1, \epsilon} & \dotsi & C_{N_1, \epsilon} \\
%                \hline
%                C_{\epsilon, 1} & \dotsi & C_{\epsilon, N_2} & 0 & \dotsi & 0 \\
%                \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
%                C_{\epsilon, 1} & \dotsi & C_{\epsilon, N_2} & 0 & \dotsi & 0 \\
%           \end{array}
%           \right]
% \end{equation}

% \begin{equation}
%      C=
%           \left[
%           \begin{array}{ccc|ccc}
%                C_{1,1} & \dotsi & C_{1, N_2} & C_{1, \epsilon} \\
%                \vdots & \ddots & \vdots & \vdots \\
%                C_{N_1, 1} & \dotsi & C_{N_1, N_2} & C_{N_1, \epsilon} \\
%           \end{array}
%           \right]
% \end{equation}


Since we don't want to restrict ourselves to the GED. We want to learn. Therefore we use a message passing network to generate node embeddings and compute pairwise distances, yielding a cost matrix like to one above. Then there are multiple paths one could take...





% set up bp cost_matrix

%% From CNN2
% Exactly solving this constrained optimization
% program would yield the exact GED solution
% (Fankhauser, Riesen, and Bunke 2011), but it is NPcomplete since it is equivalent to finding an optimal matching in a complete bipartite graph (Riesen and Bunke 2009).
% To efficiently solve the assignment problem, the Hungarian algorithm (Kuhn 1955) and the Volgenant Jonker (VJ)
% (Jonker and Volgenant 1987) algorithm are commonly used,
% which both run in cubic time. In contrast, GSimCNN takes
% advantage of the exact solutions of the instances of this problem during the training stage, and computes the approximate
% GED during testing in quadratic time, without the need for
% solving any optimization problem for a new graph pair.

% message passing formulas
