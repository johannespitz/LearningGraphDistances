\section{Introduction}

Closely following the work of Beklin et al. \cite{belkin2018a} we will investigate the generalization properties of kernel methods and compare our results to recent work in deep neural networks.

\begin{itemize}
    \item First we show experimentally that overfitted or interpolated kernel classifiers generalize well onto unseen test sets.
    \item Then we look at the proof given by Belkin et al. showing that currently known bounds are unlikely to explain this behaviour.
    \item From our final experiment we conclude that more analysis regarding the correlation of training data size, function norm, and generalization properties is required.
\end{itemize}


% Finally we conclude that it would be great if people come up with a more useful generalization bound :D.

% To summarize, in this report we will critically analyze the results of Belkin et al. and show that more theoretical and experimental work is needed to explain the generalization of overfitted classifiers.
