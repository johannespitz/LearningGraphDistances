\section{Related Work}



% approximations riba
% classical methods bai
% our method to compute exact distance
% applications

% Faced with the great significance yet huge difficulty of computing the exact GED between two graphs,
% a flurry of approximate algorithms have been proposed with a trade-off between speed and accuracy.
% However, these methods usually require rather complicated design and implementation based on discrete optimization or combinatorial search. The time complexity is usually polynomial or even sub-exponential in
% the number of nodes in the graphs, such as HED
% (Fischer et al. 2015), Hungarian (Riesen and Bunke 2009),
% VJ (Fankhauser, Riesen, and Bunke 2011), A*-Beamsearch
% (Beam) (Neuhaus, Riesen, and Bunke 2006), etc


The broad range of applicability (of graphs), and the recent success of deep learning across different tasks [vision, rl, nlp] has spurred on the research in the application of deep neural networks to graphs. Borowing the idea of convolutional layers (\citealp{alexnet2012}) from conputer vision, \cite{kipf2017} introduced Graph Convolutional Networks (GCN) which can be implemented efficiently via message passing between connected nodes. Based on this message passing mechanism researchers have improved upon classical state-of-the-art methods in node classification and graph classification [find something in the pytorch-geometric list].

Recently researchers started applying graph neural networks based on the message passing mechanism to the task of GED approximation. Bai et al. proposed in 2018 (published \citealp{bai2019}) to apply the same GCN to both graphs and aggregate the resulting node embeddings with an attention layer into two graph embedding, which can be used to compute a distance. In addition to this fully differentiable and therefore trainable part, they compute a histogram of the pairwise distances between the node embeddings to incorporate some of the local node level information. However, the histogram method is not differentiable and it is unclear if the model can properly exploit this information. Moreover, this process reduces any graph to fixed-size embeddings and is unlikely to scale to larger graphs without hand-tuning the embedding size. Despite these flaws the authors show that their method significantly outperforms classical methods while being orders of magnitude faster at inference.
% In particular they compared their results with the Hungarian algorithm (\citealp{hungarian2009}), VJ

% The first category of baselines includes three classic algorithms
% for GED computation. (1) A*-Beamsearch (Beam) [28]. It is a variant
% of the A* algorithm in sub-exponential time. (2) Hungarian [23, 35]
% and (3) Volgenant Jonker VJ [9, 18] are two cubic-time algorithms based on the Hungarian Algorithm for bipartite graph matching, and the algorithm
% of Volgenant and Jonker, respectively

Later \cite{bai2018_cnn1} proposed to compute the pairwise distances between nodes after each layer of the GCN and apply standard 2-D Convolutional Neural Networks (CNN) to these matrices. The main problems with this approach as described in their work are:
\begin{itemize}
     \item \textbf{Permutation invariance.} The ordering of the nodes in the graph represenation should not change the computed distance.
     \item \textbf{Spatial locality.} CNN's introduce a strong strucutral prior by making the assumption that nearby points are strongly realted while further apart points are not.
\end{itemize}
The authors alleviate these problems by using breadth-first-search node-ordering (\citealp{bfs2018}). They claim that nearby nodes are sorted close to each other. However, clearly this cannot be true in general for all nodes. Furthermore, the ordering is not unique, as it is not yet known if there exists an polynomial algorithm to find a canonical ordering (\citealp{canonical2016}). While this is not a principled approach the authors show an intersting connection between convolutional kernels and the optimal assignment (Section 4.1) and show strong empirical results compared to the previously mentioned classical algorithms.
% and bipartite matching (Section 4.2)

Presumalby indepently, \cite{riba2018} proposed a similar siamese network to estimate graph distances.  They apply the same GCN to both graphs generating sets $A$ and $B$ of node embeddings (one for each graph). The distance between two graphs is then computed as:
\begin{equation}
     d(g_{1}, g_{2}) = \frac{\sum_{a \in A} \inf_{b \in B} d(a, b) + \sum_{a \in A} \inf_{b \in B} d(a, b)}{\vert V_1 \vert + \vert V_2 \vert}
\end{equation}
This equation is derived as a soft variant of the Hausdorf Distance, which is also known as the Chamfer Distance in computer vision (\citealp{chamfer1977}). Instead of approximating the GED directly they validate their method on graph classification and keyword spotting datasets. Therefore, their empirical results are difficult to compare to Bai et al. (2018; 2019). This method is interesting because it generates a complete matching between nodes and might therefore scale to larger graphs without hand-tuning the embedding size. Local node structures are incorporated during the message passing, and global and local structures are relevant for the final distance.

\cite{li2019} used a similar method, called Graph Matching Network (GMN), for graph classification and detecting vulneralbe binaries by comparing control flow graphs of different compilers. The GMN also employs message passing layers to both graphs. However, they modify the message passing to allow information of one graph to flow into the node updates of the other graph. % TODO? cross-graph attention formulas
This process is fully differentiable and makes use of local node level and global graph level information but still suffers from the problem of reducing the entire graph to a single embedding vector which is expected to scale unfavourably to larger graphs. Moreover, computing the cross-graph matching vector costs $O(\vert V_1 \vert \vert V_2 \vert)$. While all previously described models have technically the same complexity ($O(\vert V_1 \vert \vert V_2 \vert + \vert E_1 \vert + \vert E_2 \vert)$), the GMN computes this cross-graph matching vector for each layer.

Note that in principle one could also apply GMN layers and then use the Soft Hausdorf Distance, making these two methods somewhat orthogonal.

\cite{fey2020_update} apply the GCN to graph matching, meaning that the network predicts pairs of nodes that correspond to each other. In particular they test their method on keypoint matching of objects in different pictures and cross-lingual knowledge graph alignment. Although this is different task their model is extremly similar to the one of \cite{riba2018}. They also apply the same GCN to both graphs (siamse network) and compute a matching. Instead of the Soft Hausdorf Distance \cite{fey2020_update} use the Sinkhorn Nomralization \cite{sinkhorn2013} on the matrix of pairwise distances. The Sinkhorn Nomralization is an iterative process that returns a doubly stochastic matrix, which is interpreted as a soft correspondence between nodes. One could immediately find a matching by taking the maximum likelihood estimate, but the authors then apply their proposed consensus steps. They generate an injective node coloring (or random node embeddings in practice) for one graph and use the soft correspondence matrix to map these embeddings to the other graph. Using the generated node embeddings they apply another GCN to both graphs yielding, after nomralization, a new soft correspondence matrix. This consensus step is repeated multiple times and the final answer is found from the last soft correspondence matrix. Unfortunately, the authors report that in practice they used the Softmax Operator on each row, instead of Sinkhorn Normalizaton due to better gradients. This can be a problem if the graphs have different number of nodes. In a toy experiment they show that with consensus steps their method is robust to node additions and removal, but it certainly is less principled because the soft corrospondence matrix and the resulting matching is not necessarily symmetric, $M(G_1, G_2) \neq M(G_2, G_1)$.

% consensus is orthogonal to our work


% maybe add how everyone computes thier pairwise distances
% < a, b >, MLP(a - b), ||a - b||_p


% In the end sinkhorn > soft hausdorf, and gmn is probably obsolete
% really fast only if we never genreate a full matching
