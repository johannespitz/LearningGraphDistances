\section{Related Work}

Researchers have already applied message passing neural networks to the task of GED approximation. Bai et al. proposed in 2018 (published \citealp{bai2019}) to apply the same network to both graphs and aggregate the resulting node embeddings with an attention layer into two global graph embeddings, which can be used to compute a distance. In addition to this fully differentiable and therefore trainable part, they compute a histogram of the pairwise distances between node embeddings to incorporate some of the local node level information. However, the histogram method is not differentiable and it is unclear if the model can properly exploit this information. Moreover, this process reduces any graph to fixed-size embeddings and is unlikely to scale to larger graphs without hand-tuning the embedding size. Despite these flaws the authors show that their method significantly outperforms classical methods while being orders of magnitude faster at inference.
% In particular they compared their results with the Hungarian algorithm (\citealp{hungarian2009}), VJ
% The first category of baselines includes three classic algorithms
% for GED computation. (1) A*-Beamsearch (Beam) [28]. It is a variant
% of the A* algorithm in sub-exponential time. (2) Hungarian [23, 35]
% and (3) Volgenant Jonker VJ [9, 18] are two cubic-time algorithms based on the Hungarian Algorithm for bipartite graph matching, and the algorithm
% of Volgenant and Jonker, respectively

Later \cite{bai2018_cnn1} proposed to apply, instead of a histogram, a standard 2-D Convolutional Neural Networks (CNN) to the matrix of pairwise distances. The main problems with this approach as described in their work are:
\begin{itemize}
     \itemsep0em
     \item \textbf{Permutation invariance.} The ordering of the nodes in the graph representation should not change the computed distance.
     \item \textbf{Spatial locality.} CNN's introduce a strong structural prior by making the assumption that nearby points are strongly related while further apart points are not.
\end{itemize}
The authors alleviate these problems by using breadth-first-search node-ordering (\citealp{bfs2018}). They claim that nearby nodes are sorted close to each other. However, there are cases where that does not work for all nodes. Furthermore, the ordering can not be guaranteed to be unique. In fact it is not yet known if there exists an polynomial algorithm to find a canonical ordering (\citealp{canonical2016}) and therefore the predictions can depend on the representation of the graphs. However, the authors show an interesting connection between convolutional kernels and the optimal assignment problem, and show strong empirical results compared to classical algorithms.

Independently, \cite{riba2018} proposed a similar model to estimate graph distances. They apply the same message passing network to both graphs generating sets $A$ and $B$ of node embeddings (one for each graph). The distance between two graphs is then computed as:
\begin{equation}
     d(G_{1}, G_{2}) = \frac{\sum_{a \in A} \inf_{b \in B} d(a, b) + \sum_{a \in A} \inf_{b \in B} d(a, b)}{N_1 + N_1}
\end{equation}
This equation is derived as a soft variant of the hausdorf distance, which is also known as the chamfer distance in computer vision (\citealp{chamfer1977}). Instead of approximating the GED directly they validate their method on graph classification and keyword spotting datasets. Therefore, their empirical results are difficult to compare to Bai et al. (2018; 2019). This method is interesting because it generates a complete matching between nodes and might therefore scale to larger graphs without hand-tuning the embedding size. Local node structures are incorporated during the message passing, and global and local structures are relevant for the final distance.

\cite{li2019} used a similar method, called Graph Matching Network (GMN), for graph classification and detecting vulnerable binaries by comparing control flow graphs of different compilers. The GMN also employs message passing layers to both graphs. However, they modify the message passing to allow information of one graph to flow into the node updates of the other graph. % cross-graph attention formulas?
This process is fully differentiable and makes use of local node level and global graph level information but still suffers from the problem of reducing the entire graph to a single embedding vector which is expected to scale unfavorably to larger graphs. Moreover, computing the cross-graph matching vector costs $O(N_1 N_2)$. While all previously described models have technically the same complexity ($O(N_1 N_2 + \vert E_1 \vert + \vert E_2 \vert)$), the GMN computes this cross-graph matching vector for each layer.

% Note that in principle one could also apply GMN layers and then use the Soft Hausdorf Distance, making these two methods somewhat orthogonal.

\cite{fey2020_update} apply the message passing networks to graph matching. In particular they test their method on keypoint matching of objects in different pictures and cross-lingual knowledge graph alignment. Although this task is different to the previously mentioned works, their model is extremely similar to the one of \cite{riba2018}. They also apply the same network to both graphs and compute a matching. Instead of the chamfer distance \cite{fey2020_update} use the sinkhorn normalization \cite{sinkhorn2013} on the matrix of pairwise distances. The sinkhorn normalization returns a doubly stochastic matrix, which is interpreted as a soft correspondence between nodes. One could immediately find a matching by taking the maximum likelihood estimate, but the authors then apply their proposed consensus steps. They generate an injective node coloring (or random node embeddings in practice) for one graph and use the soft correspondence matrix to map these embeddings to the other graph. Using the generated node embeddings they apply another message passing network to both graphs yielding, after normalization, a new soft correspondence matrix. This consensus step is repeated multiple times and the final answer is found from the last soft correspondence matrix. Unfortunately, the authors report that in practice they had to use the softmax operator on each row, instead of sinkhorn normalization due to lacking gradient information. However, when using the softmax the model is not necessarily symmetric anymore, $M(G_1, G_2) \neq M(G_2, G_1)$.

% This can be a problem if the graphs have different number of nodes.
% In a toy experiment they show that with consensus steps their method is robust to node additions and removal, but it certainly is less principled because the soft corrospondence matrix and the resulting matching is not necessarily symmetric, $M(G_1, G_2) \neq M(G_2, G_1)$.

% consensus is orthogonal to our work


% maybe add how everyone computes thier pairwise distances
% < a, b >, MLP(a - b), ||a - b||_p


% In the end sinkhorn > soft hausdorf, and gmn is probably obsolete
% really fast only if we never genreate a full matching
