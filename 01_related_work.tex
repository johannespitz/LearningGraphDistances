\section{Related Work}

Except from the mainly analyzed work by Belkin et al. \cite{belkin2018a} most research regarding the surprising generalization properties of overfitted classifiers is done in deep neural networks. For example, Poggio et al. \cite{poggio2018} showed that properties of stochastic gradient descent (SGD) for linear networks also hold for deep non-linear networks. Firstly, SGD enforces an implicit form of regularization and converges to the minimum norm solution, and secondly for classification tasks this minimum norm solution is also the maximum margin solution. Therefore, it yields good generalization results on low noise datasets. Bartlett et al. \cite{bartlett2017} showed that SGD selects predictors whose complexity scales with the difficulty of the learning task and conclude that the good generalization is a property of the optimization algorithm. Zhang et al. \cite{zhang2017} successfully trained state-of-the-art convolutions neural networks (CNN) on randomly labeled datasets close to zero classification error and found that choosing the right model family or regularization techniques doesn't explain the small difference between training and test performance on correctly labeled datasets.
