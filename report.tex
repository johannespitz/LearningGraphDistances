\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{seminar}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{float}
\usepackage{hyperref}


\usepackage{mathrsfs}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{appendix}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\pagestyle{fancy}


%You can add theorem-like environments (e.g. remark, definition, ...) if you want
\newtheorem{theorem}{Theorem}

\title{Kernel methods and Deep Learning II} % Replace with your title
\author{Johannes Pitz} % Replace with your name
\institute{\textit{Seminar: Optimization and Generalization in Deep Learning}}

\makeatletter
\let\runauthor\@author
\let\runtitle\@title
\makeatother
\lhead{\runauthor}
\rhead{\runtitle}


\begin{document}

\maketitle

\begin{abstract}

In deep neural networks often models which were trained until they reach zero training error with little to no regularization perform better on unseen test sets than the same model trained with early stopping and/or explicit regularization. This effect is contradicting the general wisdom taught in machine learning classes, that overfitting will lead to decreasing test performance. It is not well understood, why these generalization properties can be observed. 

This report will show that the mentioned effect is not unique to deep neural networks, but can also be observed in kernel methods. We will show that kernel methods can be viewed as two-layer neural networks. Therefore it might be advantageous to closely investigate the much simpler kernel methods before trying to understand more complex deep architectures. Moreover, we argue that the effect might not be a property of the model alone, but instead depend on the data, optimizer and model.

% (127 words) + last sentence

\end{abstract}

\input{01_introduction.tex}

\input{01_related_work.tex}

\input{02_approach.tex}

\input{03_results.tex}

\input{04_conclusion.tex}

\bibliographystyle{plain}
\bibliography{egbib}

\newpage

\input{05_appendix.tex}


\end{document}
