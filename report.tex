\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{seminar}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{float}
\usepackage{hyperref}

\usepackage[round]{natbib}   % omit 'round' option if you prefer square brackets

\usepackage{hhline}
\usepackage{multirow}

\usepackage{mathrsfs}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{appendix}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\pagestyle{fancy}


%You can add theorem-like environments (e.g. remark, definition, ...) if you want
\newtheorem{theorem}{Theorem}

\title{Learning Graph Distances} % Replace with your title
% \newcommand{\shorttitle}{\title}
% \shorttitle{Learning graph distances}
\author{Johannes Pitz} % Replace with your name
\institute{\textit{Guided Research: Data Analytics and Machine Learning Group  \protect\\ TUM Department of Informatics}}

\makeatletter
\let\runauthor\@author
\let\runtitle\@title
\makeatother
\lhead{\runauthor}
\rhead{\runtitle}


\begin{document}

\title{Learning Graph Distances \protect\\ via Graph Neural Networks and Node Matching}
\maketitle

\begin{abstract}


Computing meaningful distances between graphs is often difficult due to the combinatorial explosion of possible transformations on graphs. Recently, researchers started using graph neural networks in an attempt to replace classical search algorithms with learning methods. However, these learning methods often rely on fixed size graph embeddings, which prevent them from scaling to larger graphs. We propose to use soft matchings between nodes to overcome this problem. Our novel combination of a classical cost matrix, motivated by bipartite graph matching algorithms, and the modern Sinkhorn distance shows state-of-the-art results for predicting the ubiquitous graph edit distance. Additionally, we report strong empirical results at graph matching with the same model. Our implementation\footnote{\url{https://gitlab.lrz.de/gdn/graph-distance}} efficiently handles sparse inputs and large real-world graphs.

% of pairwise distances between nodes in the form

% However, these learning methods often neglect the

% Graph neural networks suffer bottleneck problems
% We propose to use soft matching between nodes, using a cost matrix, motivated by bipartite graph matching and Sinkhorn distance.
% We show empirically that our model improves upon state-of-the-art methods on the task of predicting the graph edit distance
% and demonstrate that it can be used for graph matching.
% Our sparsity-aware implementation\footnote{} scales well to large real-world graphs.



\end{abstract}

\input{01_introduction.tex}

\section{Background}

Let $\mathcal{G}$ be a set of graphs, where each graph $G = \{V, E\} \in \mathcal{G}$ with nodes $V$ and edges $E$ is drawn from a common distribution. The nodes and edges may have arbitrary features. Usually we expect the number of edges $\vert E \vert$ to be much smaller than the number of all possible edges, i.e. $N^2 = \vert V \vert ^2$. Possible distributions include chemical molecules, control flow graphs, or synthetic preferential attachment graphs (\citealp{pref_att2002}).

Graph neural networks are based on message passing:
message
propagate
update.

\input{01c_related_work.tex}

\input{01b_derivation.tex}

\input{02_setup.tex}

\input{03_results.tex}

\input{04_conclusion.tex}

\input{05_outlook.tex}

\newpage

\bibliographystyle{plainnat}
\bibliography{egbib}

% \cite    -> Name (year)
% \citealp -> Name, year

\newpage

\input{05_appendix.tex}


\end{document}
