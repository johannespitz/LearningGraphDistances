
\subsection{Experiment 1}

 Looking at Figure \ref{fig:1} one can see that for the \textit{overfitted} classifiers early stopping doesn't offer any benefit on the test error even though they are heavily (over)fitting to the training data. Moreover even the interpolated classifier achieves the same test error. This parallels findings in deep neural networks, and therefore shows that the generalization effect is not inherent to the depth of the network (i.e. Zhang et al. \cite{zhang2017}).

 Note: Unlike on MNIST, we weren't immediately successfully to reproduce the results of Belkin et al. on CIFAR-10, however using all features (i.e. not converting the pictures to grey scale) we attained the desired effect.

\begin{figure}[!htb]
\begin{subfigure}{0.49\textwidth}
% \includegraphics[width=\linewidth]{img/Figure1_MNIST.png}
\caption{MNIST} \label{fig:1a}
\end{subfigure}
\hspace*{\fill} % separation between the subfigures
\begin{subfigure}{0.49\textwidth}
% \includegraphics[width=\linewidth]{img/Figure1_color.png}
\caption{CIFAR-10} \label{fig:1b}
\end{subfigure}
\caption{Comparison between overfitted and interpolated classifiers.} \label{fig:1}
\end{figure}
