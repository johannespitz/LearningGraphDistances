\section{Setup}

% Siamese GCN.
% Aggregate
% Set up cost matrix (bp vs. normal, scale with $\alpha$).
% norms
% Compute explicit matching with sinkhorn (use cost matrix as kernel, can tune $\lambda$).
% Use optimal matching for backpropagation.
% Multiply matching with cost matrix.

\subsection{Experiment 1}

For our experiments we use the a siamese network structure with multiple shared message passing layers, followed by the matching step.

% TODO leaky relu

Let $d_h$ be an arbitrary embedding size, $d_{\text{node}}$ be the number of node features of the input graphs, $d_{\text{edge}}$ be the number of edge features, $W_l \in \mathbb{R}^{d_h \times d_h}$, $b_l \in   \mathbb{R}^{d_h}$be a weight matrix and bias, $h_i^l \in \mathbb{R}^{d_h}$ be the embedding of node $i$ at layer $l$ for $l > 0$, $\sigma(\cdot)$ be an elementwise non-linearity, $E \in \mathbb{R}^{d_{\text{edge}} \times d_h \times d_h}$ be a weight tensor ($h_i^0 = v_i \in \mathbb{R}^{d_{\text{node}}}$, $W_0 \in \mathbb{R}^{d_{\text{node}} \times d_h}$, $b_0 \in \mathbb{R}^{d_{\text{node}}}$).

In our first experiment we use the following message passing function:
\begin{equation}
     h_i^{l} = \sigma(W_{l} h_i^{l-1} + b_l) + \sum_{j \rightarrow i} \sigma(W_{l} h_j^{l-1} + b_l) E_{e_{j \rightarrow i}}
\end{equation}
where $E_{e_{j \rightarrow i}} \in \mathbb{R}^{d_h \times d_h}$ is the weight tensor indexed by the discrete feature of the edge connecting node $j$ with node $i$. As activation function $\sigma(\cdot)$ we use the leaky\_relu non-linearity.

To compute the entries of the cost matrix we concatenate all node embeddings and take the norm of pairwise distances across both graphs, or the embeddings scaled by a trainable $\alpha \in \mathbb{R}^{d_\text{final}}$:
\begin{equation}
     \begin{gathered}
          h_i^\text{final} = [h_L^i ; \dots ; h_1^i ; \sigma(W_{1} h_i^{0} + b_0)] \in \mathbb{R}^{d_\text{final}}\\
          C_{i,j} = \norm{h_i^\text{final} - h_j^\text{final}} \quad
          C_{i, \epsilon} = \norm{ \alpha h_i^\text{final}} \quad
          C_{\epsilon, j} = \norm{ \alpha h_j^\text{final}} \quad i \in \{1 \dots N_1\}, j \in \{1 \dots N_2\}
     \end{gathered}
\end{equation}

In our experiment we tested three different norms: the euclidean norm ($p=2$), the manhattan norm ($p=1$), and a simple two layer perceptron (MLP). The first layer does not change the embedding size, then we apply a relu non-linearity. The second layer reduces the embedding size to a single number to which we apply the softplus non-linearity, $\ln(1 + e^x)$.

As for the matrix $C$ itself we tested two variants. The first one we call BP-Distance on account of bipartite graph matching. Due to implementation advantages we decided to pad the pairwise distances with rows and columns of $C_{i, \epsilon}, C_{\epsilon, j}$ respectively, instead of the $\infty$-values typically used.
\begin{equation}
     C_\text{BP}=
          \left[
          \begin{array}{ccc|ccc}
               C_{1,1} & \dotsi & C_{1, N_2} & C_{1, \epsilon} & \dotsi & C_{1, \epsilon} \\
               \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
               C_{N_1, 1} & \dotsi & C_{N_1, N_2} & C_{N_1, \epsilon} & \dotsi & C_{N_1, \epsilon} \\
               \hline
               C_{\epsilon, 1} & \dotsi & C_{\epsilon, N_2} & 0 & \dotsi & 0 \\
               \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
               C_{\epsilon, 1} & \dotsi & C_{\epsilon, N_2} & 0 & \dotsi & 0 \\
          \end{array}
          \right]
     \in \mathbb{R}^{(N_1 + N_2) \times (N_1 + N_2)}
\end{equation}
This does lead to different results after sinkhorn normalization due to the entropy regularization, but the network can easily adapt by increasing $\norm{\alpha}$. Therefore, this modification should not affect our trainable model. The second variant of the cost matrix is the smallest square sub-matrix of the one above that contains all pairwise distances, such that \mbox{$C_\text{no-BP} \in \mathbb{R}^{\max({N_1, N_2}) \times \max({N_1, N_2})}$}.

The final distance $d$ is then computed and trained via the mean squared error between target distance and prediction:
\begin{equation}
     \begin{gathered}
          M = \text{sinkhorn}(e^{-\frac{C}{\lambda}}) \\
          d = \langle M, C \rangle_\mathrm{F} \\
          \text{loss} = (d - d_\text{target})^2
     \end{gathered}
\end{equation}

Implementation. Our GNN is implemented with PyTorch-geometric (\citealp{pytorchgeometric}) and can process sparse mini-batches with parallel GPU acceleration. We have two implementations for the matching. The first version pads matrices to form batches and can be used in cases where all graphs have similar number of nodes. The second version uses sparse matrices in COOrdinate format, which less efficient with evenly sized graphs but can be crucial for parallelizing workloads with a few large graphs.

\subsection{Experiment 2}

\cite{fey2020_update} have recently applied a very similar model to ours to learn graph matching. They also use a GNN to generate node embeddings and compute a cost matrix of pairwise distances. Their main contribution is the following consensus method. After normalizing the cost matrix, they use this matching to map randomly generated features of one graph to the other. Then they apply a different GNN to the graphs with new features and repeat the process multiple times. The loss is the negative log likelihood of the soft matching given some ground truth matching (following \citealp{wang2019}). And the final performance is measured by how often the row-wise maximum in the soft matching yields the correct match (Hits@1).

To align our model as much as possible with their work we use the same SplineCNN (\citealp{spline2018}) with the same embedding size and number of layers to generate node embeddings. We use $p=2$ for our norm, and the no-BP variant of the cost matrix. For numeric stabilization we scaled the kernel matrix by the sum of all entries divided by the number of entries: $\text{reg} = \frac{\norm{C_l}_1}{\max(n1,n2)^2}$. We also implemented the consensus method for a more detailed comparison.

The main difference between our models is that \cite{fey2020_update} normalize the cost matrix $C$ instead of kernel matrix $e^{-\frac{C}{\lambda}}$. Consequently, they report bad gradients with sinkhorn normalization and use row-wise sofmax instead. For their cost matrix they only use the pairwise distances between nodes without padding with norms, which should not be a problem for tasks where each node in the first graph has a match in the second graph, but could be problematic in other settings. Below we have a complete list of the differences between our models:

\begin{alignat*}{3}
     & && \quad \text{\cite{fey2020_update}} && \quad \text{Ours} \\
     &C_0 &&= h_i \cdot h_j &&= \norm{h_i - h_j}_2 \qquad \text{(fill with norms)} \\
     &C_l &&= C_{l - 1} + \text{MLP}(h_i - h_j) &&= \beta_l C_0 + (1 - \beta_l) \text{MLP}(h_i - h_j) \qquad \text{(keep old norms scaled by} \alpha^2 \text{)} \\
     &M_l &&= \text{softmax}(C_l) &&= \text{sinkhorn}(e^{-\frac{C_l * \text{reg}}{\lambda}}) \\
\end{alignat*}

% We also implemented a consensus step based on their work. We use instead of
% \\
% $C_0 = h_s \cdot h_t$\\
% $C_0 = \norm{h_s - h_t}_2$ fill with norms\\
% $C_l = C_{l - 1} + \text{MLP}(h_s - h_t)$\\
% $C_l = \beta_l C_0 + (1 - \beta_l) \text{MLP}(h_s - h_t)$ keep old norms scaled by $\alpha^2$\\
% $M_l = \text{softmax}(C_l)$ \\
% $M_l = \text{sinkhorn}(e^{-\frac{C_l * \text{reg}}{\lambda}}) \quad \text{reg} = \frac{\norm{C_l}_1}{\max(n1,n2)^2}$\\
