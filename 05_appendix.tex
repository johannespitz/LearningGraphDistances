\appendix
\appendixpage

In this report I summarize the work I did during my guided research. The main part of the report focuses on findings that are relevant to a potential paper submission, but here in the appendix I will add some information about the datasets we used, and some we did not use (for completeness).


\section{Graph Edit Distance Datasets}
\begin{table}[htbp]
    \addtolength{\tabcolsep}{-1pt}
    \fontsize{9pt}{10.25pt}\selectfont
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|l|c|c|c|c|c|c|c|}
        \hline
        Dataset & Node Feat & Edge Feat & \#Train & \#Val/Test & Max Nodes & Avg Nodes & Avg Edges \\
        \hline
        Pref-Att & 6 & 4 & 144 & 48 & 30 & 20.6 & 75.4 \\ % edges mean 75.4
        \hline
        Aids & 53 & 4 & 144 & 48 & 30 & 20.6 & 44.6 \\ % edges mean 44.6
        \hline
    \end{tabular}
    \caption{Datasets}
    \label{tab:ex1-data}
\end{table}

Note \#Train and \#Val/Test are the number of graphs. We train and validate/test on all possible pairs of graphs, except the pairs where a graph is paired with itself.

\section{Yeast Dataset}

The Yeast Dataset (\url{https://www3.nd.edu/~cone/MAGNA++/}), used by \cite{yeast2019}, contains the protein-protein interaction (PPI) network of yeast and noise versions thereof. The base network contains 1,004 proteins and 4,920 high-confidence interactions. The noisy versions contain additional low-confidence interactions.

The first obstacle with this dataset would have been the generation of train/val/test splits.  Moreover, we could not have compared our results directly with \cite{yeast2019} because they learn unsupervised. The second obstacle is that the dataset contains no node or edge features. That is mostly a problem because GNN's are particularly good at using node, and also edge features. Therefore, it is questionable if a  model based on GNN's would perform impressive enough. In the end we decided not to use this dataset, also because we discovered the paper by \cite{fey2020_update} who use plenty of graph matching datasets that fit much better to our model.

\section{Control Flow Dataset}\

\cite{li2019} used control flow graphs to evaluate their model. They used metric learning (margin-loss with paris, and triplets) to recognize a function compiled from different compliers as similar, and different functions as not similar. They didn't share they dataset of FFmpeg function with us, but referred us to a github repository, which I then forked (\url{https://github.com/johannespitz/functionsimsearch/tree/master}) and used to generate the UnRAR datasets, which (at the time of writing) can be found at '/nfs/students/pitz/cfgs' on the file server. To generate the graphcollection's I used the notebook 'cfgs.ipynb' in the graph-distance repository. Note: use the .pickle version because it is much faster than .npz.

Currently we have a few different options for train/val/test splits. The simplest split is to use train\_all/val/test, which is an 80/10/10 split in terms of functions (not graphs). The attraction list contains all pairs of graphs that are from the same function, and the repulsion list contains as many negative samples. The train\_all split is then further split in: Option1, train\_across/val\_across/test\_acorss. Here out of the n graphs of any function we pick one to be the test graph, and all pairs between it and the remaining n-1 graphs are placed in the attraction list. Of the n-1 we then draw a validation graph and again all pairs between it and the remaining n-2 graphs are placed in the attraction list. The attraction list of the train\_across split contains then all possible pairs of the remaining n-2 graphs. Option2, train12/val1/val2/test1/test2. Here we split train\_all as chunk/val2/test2 (70/15/15) in terms of graphs (not functions), while ensuring that at least one graph of each function is contained the chunk. We then generate all positive pairs, but for the chunk we split those again into train12/val1/test1 (80/10/10) ensuring that every graph is in train12 at least once. All repulsion list for both options contain as many negative samples as the corresponding attraction list. Triplets are only available with the simple  train\_all/val/test (80/10/10) split.

Our model can be trained with pairs or triplets as it is. I did not include any results with the dataset in this report because the baselines cannot without putting in additional work.
